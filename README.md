# LLM-Security-Guardrails-Lab
Lab for implementing and testing security guardrails for Large Language Models, including prompt injection prevention, retrieval poisoning mitigation, and output filtering patterns.
